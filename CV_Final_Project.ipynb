{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoYaI93eXhOi"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FTqB0x-z6ej",
        "outputId": "4d7eae6a-8daf-41de-8bc2-c9ee1e708a72"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install h5py\n",
        "!pip install scipy\n",
        "!pip install Pillow\n",
        "!pip install opencv-python\n",
        "!pip install pycocotools\n",
        "!pip install scikit-image\n",
        "import torch, torchvision\n",
        "import torchvision.transforms as T\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob\n",
        "import zipfile\n",
        "from PIL import Image , ImageOps\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import argparse\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "from __future__ import print_function, absolute_import, division\n",
        "from collections import namedtuple\n",
        "from pycocotools.coco import COCO\n",
        "import skimage.color\n",
        "import skimage.io\n",
        "import skimage.transform\n",
        "from pycocotools import mask as maskUtils\n",
        "import matplotlib.pyplot as plt\n",
        "import colorsys\n",
        "from matplotlib import patches,  lines\n",
        "from skimage.measure import find_contours\n",
        "from matplotlib.patches import Polygon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3hT7wo-iHAJ",
        "outputId": "15946493-fab1-4ef1-eb95-72dafd01f906"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/bkwalsh/CV-final-project.git\n",
        "%cd CV-final-project/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDYsZHD_VFYh",
        "outputId": "d888b72a-2d67-43f9-e535-546662d221df"
      },
      "outputs": [],
      "source": [
        "# loading Cityscapes data through Bash Script (as per https://github.com/cemsaz/city-scapes-script)\n",
        "!wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=bkwalsh&password=Aaaaaaaaa123456!!&submit=Login' https://www.cityscapes-dataset.com/login/\n",
        "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1\n",
        "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGSSTSe3pDSB",
        "outputId": "4f361b7d-ddd3-4680-a91c-c2bce7f2286b"
      },
      "outputs": [],
      "source": [
        "%cd /content/CV-final-project//\n",
        "!mkdir conversion\n",
        "%cd conversion\n",
        "!mkdir data\n",
        "%cd data\n",
        "!mkdir cityscapes\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF0gyDz-RyXG"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('/content/CV-final-project/gtFine_trainvaltest.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/CV-final-project/conversion/data/cityscapes')\n",
        "\n",
        "with zipfile.ZipFile('/content/CV-final-project/leftImg8bit_trainvaltest.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/CV-final-project/conversion/data/cityscapes/gtFine')\n",
        "\n",
        "# maybe move data here manually instead\n",
        "with zipfile.ZipFile('/content/CV-final-project/leftImg8bit_trainvaltest.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/CV-final-project/conversion/data/cityscapes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R32vcph0Dxs7"
      },
      "outputs": [],
      "source": [
        "!cd /content/CV-final-project/conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9StGJ7BSD_0"
      },
      "source": [
        "# Object / Label Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjp1Rv1f0yOy"
      },
      "outputs": [],
      "source": [
        "# built from CityScapes Instance utils page (https://github.com/mcordts/cityscapesScripts/blob/a7ac7b4062d1a80ed5e22d2ea2179c886801c77d/cityscapesscripts/evaluation/instance.py)\n",
        "\n",
        "class Instance(object):\n",
        "    instID     = 0\n",
        "    labelID    = 0\n",
        "    pixelCount = 0\n",
        "    medDist    = -1\n",
        "    distConf   = 0.0\n",
        "\n",
        "    def __init__(self, imgNp, instID):\n",
        "        if (instID == -1):\n",
        "            return\n",
        "        self.instID     = int(instID)\n",
        "        self.labelID    = int(self.getLabelID(instID))\n",
        "        self.pixelCount = int(self.getInstancePixels(imgNp, instID))\n",
        "\n",
        "    def getLabelID(self, instID):\n",
        "        if (instID < 1000):\n",
        "            return instID\n",
        "        else:\n",
        "            return int(instID / 1000)\n",
        "\n",
        "    def getInstancePixels(self, imgNp, instLabel):\n",
        "        return (imgNp == instLabel).sum()\n",
        "\n",
        "    def toJSON(self):\n",
        "        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)\n",
        "\n",
        "    def toDict(self):\n",
        "        buildDict = {}\n",
        "        buildDict[\"instID\"]     = self.instID\n",
        "        buildDict[\"labelID\"]    = self.labelID\n",
        "        buildDict[\"pixelCount\"] = self.pixelCount\n",
        "        buildDict[\"medDist\"]    = self.medDist\n",
        "        buildDict[\"distConf\"]   = self.distConf\n",
        "        return buildDict\n",
        "\n",
        "    def fromJSON(self, data):\n",
        "        self.instID     = int(data[\"instID\"])\n",
        "        self.labelID    = int(data[\"labelID\"])\n",
        "        self.pixelCount = int(data[\"pixelCount\"])\n",
        "        if (\"medDist\" in data):\n",
        "            self.medDist    = float(data[\"medDist\"])\n",
        "            self.distConf   = float(data[\"distConf\"])\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"(\"+str(self.instID)+\")\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSUwj9IH0iKL"
      },
      "outputs": [],
      "source": [
        "# built from CityScapes Labels utils page (https://github.com/mcordts/cityscapesScripts/blob/a7ac7b4062d1a80ed5e22d2ea2179c886801c77d/cityscapesscripts/helpers/labels.py#L109)\n",
        "\n",
        "Label = namedtuple( 'Label' , ['name','id','trainId','category','categoryId','hasInstances','ignoreInEval','color',] )\n",
        "\n",
        "\n",
        "labels = [\n",
        "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
        "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
        "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
        "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
        "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
        "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
        "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
        "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
        "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
        "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
        "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
        "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
        "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
        "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
        "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
        "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
        "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
        "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
        "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
        "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
        "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
        "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
        "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
        "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
        "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
        "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
        "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
        "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
        "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
        "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlL6HkStz2bm"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Prv5dEpaJ6G"
      },
      "outputs": [],
      "source": [
        "# built from DETR utils page (https://github.com/facebookresearch/Detectron/tree/master/tools)\n",
        "\n",
        "def cityInstaceToPolygon(files):\n",
        "    id2label     = { label.id : label for label in labels}\n",
        "    imgCount     = 0\n",
        "    instanceDict = {}\n",
        "\n",
        "    if not isinstance(files, list):\n",
        "        files = [files]\n",
        "\n",
        "    for fileName in files:\n",
        "        img = Image.open(fileName)\n",
        "        imgNp = np.array(img)\n",
        "        instances = {}\n",
        "        for label in labels:\n",
        "            instances[label.name] = []\n",
        "        for instanceId in np.unique(imgNp):\n",
        "            if instanceId < 1000:\n",
        "                continue\n",
        "            instanceObj = Instance(imgNp, instanceId)\n",
        "            instanceObj_dict = instanceObj.toDict()\n",
        "\n",
        "            if id2label[instanceObj.labelID].hasInstances:\n",
        "                mask = (imgNp == instanceId).astype(np.uint8)\n",
        "                contour, hier = cv2.findContours(\n",
        "                    mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "                polygons = [c.reshape(-1).tolist() for c in contour]\n",
        "                instanceObj_dict['contours'] = polygons\n",
        "\n",
        "            instances[id2label[instanceObj.labelID].name].append(instanceObj_dict)\n",
        "\n",
        "        instanceDict[fileName] = instances\n",
        "        imgCount += 1\n",
        "\n",
        "    return instanceDict\n",
        "\n",
        "\n",
        "def poly_to_box(poly):\n",
        "    \"\"\"Convert a polygon into a tight bounding box.\"\"\"\n",
        "    x0 = min(min(p[::2]) for p in poly)\n",
        "    x1 = max(max(p[::2]) for p in poly)\n",
        "    y0 = min(min(p[1::2]) for p in poly)\n",
        "    y1 = max(max(p[1::2]) for p in poly)\n",
        "    box_from_poly = [x0, y0, x1, y1]\n",
        "    return box_from_poly\n",
        "\n",
        "def xyxy_to_xywh(xyxy_box):\n",
        "    xmin, ymin, xmax, ymax = xyxy_box\n",
        "    TO_REMOVE = 1\n",
        "    xywh_box = (xmin, ymin, xmax - xmin + TO_REMOVE, ymax - ymin + TO_REMOVE)\n",
        "    return xywh_box\n",
        "\n",
        "\n",
        "def conversion():\n",
        "    data_dir = \"data/cityscapes\"\n",
        "    out_dir = \"data/cityscapes/annotations\"\n",
        "    sets = [\n",
        "        'leftImg8bit/train',\n",
        "        'leftImg8bit/val'\n",
        "    ]\n",
        "\n",
        "    ann_dirs = [\n",
        "        'gtFine/train',\n",
        "        'gtFine/val',\n",
        "    ]\n",
        "\n",
        "    json_name = 'instancesonly_filtered_%s.json'\n",
        "    polygon_json_file_ending = '_polygons.json'\n",
        "    img_id = 0\n",
        "    ann_id = 0\n",
        "    cat_id = 1\n",
        "    category_dict = {}\n",
        "\n",
        "\n",
        "    for data_set, ann_dir in zip(sets, ann_dirs):\n",
        "        print('Starting %s' % data_set)\n",
        "        ann_dict = {}\n",
        "        images = []\n",
        "        annotations = []\n",
        "\n",
        "        for root, _, files in os.walk(os.path.join(data_dir, ann_dir)):\n",
        "            for filename in files:\n",
        "                if filename.endswith(polygon_json_file_ending):\n",
        "\n",
        "                    if len(images) % 50 == 0:\n",
        "                        print(\"Processed %s images, %s annotations\" % (len(images), len(annotations)))\n",
        "\n",
        "                    json_ann = json.load(open(os.path.join(root, filename)))\n",
        "\n",
        "                    image = {}\n",
        "                    image['id'] = img_id\n",
        "                    img_id += 1\n",
        "                    image['width'] = json_ann['imgWidth']\n",
        "                    image['height'] = json_ann['imgHeight']\n",
        "                    image['file_name'] = os.path.join(\"leftImg8bit\",\n",
        "                                                      data_set.split(\"/\")[-1],\n",
        "                                                      filename.split('_')[0],\n",
        "                                                      filename.replace(\"_gtFine_polygons.json\", '_leftImg8bit.png'))\n",
        "                    image['seg_file_name'] = filename.replace(\"_polygons.json\", \"_instanceIds.png\")\n",
        "                    images.append(image)\n",
        "\n",
        "                    fullname = os.path.join(root, image['seg_file_name'])\n",
        "                    objects = cityInstaceToPolygon([fullname])[fullname]\n",
        "\n",
        "                    for object_cls in objects:\n",
        "                        if object_cls not in ['person','rider','car','truck','bus','train','motorcycle','bicycle',]:\n",
        "                            continue\n",
        "\n",
        "                        for obj in objects[object_cls]:\n",
        "                            len_p = [len(p) for p in obj['contours']]\n",
        "                            if (min(len_p) <= 4) or (obj['contours'] == []):\n",
        "                                continue\n",
        "\n",
        "                            ann = {}\n",
        "                            ann['id'] = ann_id\n",
        "                            ann_id += 1\n",
        "                            ann['image_id'] = image['id']\n",
        "                            ann['segmentation'] = obj['contours']\n",
        "\n",
        "                            if object_cls not in category_dict:\n",
        "                                category_dict[object_cls] = cat_id\n",
        "                                cat_id += 1\n",
        "                            ann['category_id'] = category_dict[object_cls]\n",
        "                            ann['iscrowd'] = 0\n",
        "                            ann['area'] = obj['pixelCount']\n",
        "\n",
        "                            xyxy_box = poly_to_box(ann['segmentation'])\n",
        "                            xywh_box = xyxy_to_xywh(xyxy_box)\n",
        "                            ann['bbox'] = xywh_box\n",
        "\n",
        "                            annotations.append(ann)\n",
        "\n",
        "        ann_dict['images'] = images\n",
        "        categories = [{\"id\": category_dict[name], \"name\": name} for name in category_dict]\n",
        "        ann_dict['categories'] = categories\n",
        "        ann_dict['annotations'] = annotations\n",
        "        print(\"Num categories: %s\" % len(categories))\n",
        "        print(\"Num images: %s\" % len(images))\n",
        "        print(\"Num annotations: %s\" % len(annotations))\n",
        "        if not os.path.exists(os.path.abspath(out_dir)):\n",
        "            os.mkdir(os.path.abspath(out_dir))\n",
        "        with open(os.path.join(out_dir, json_name % ann_dir.replace(\"/\", \"_\")), 'w') as outfile:\n",
        "            outfile.write(json.dumps(ann_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4jMuOyd0aWj",
        "outputId": "87bb1bf9-a6ae-4390-85ac-a2d930bdac4a"
      },
      "outputs": [],
      "source": [
        "conversion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qbq1UHAA8Jx"
      },
      "source": [
        "# COCO Dataset/ Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXTTJu4CAFQi"
      },
      "outputs": [],
      "source": [
        "# COCO Dataset UTILS (adopted from https://github.com/cocodataset/cocoapi)\n",
        "class Dataset(object):\n",
        "    \"\"\"The base class for dataset classes.\n",
        "    To use it, create a new class that adds functions specific to the dataset\n",
        "    you want to use. For example:\n",
        "\n",
        "    class CatsAndDogsDataset(Dataset):\n",
        "        def load_cats_and_dogs(self):\n",
        "            ...\n",
        "        def load_mask(self, image_id):\n",
        "            ...\n",
        "        def image_reference(self, image_id):\n",
        "            ...\n",
        "\n",
        "    See COCODataset and ShapesDataset as examples.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, class_map=None):\n",
        "        self._image_ids = []\n",
        "        self.image_info = []\n",
        "        # Background is always the first class\n",
        "        self.class_info = [{\"source\": \"\", \"id\": 0, \"name\": \"BG\"}]\n",
        "        self.source_class_ids = {}\n",
        "\n",
        "    def add_class(self, source, class_id, class_name):\n",
        "        assert \".\" not in source, \"Source name cannot contain a dot\"\n",
        "        # Does the class exist already?\n",
        "        for info in self.class_info:\n",
        "            if info['source'] == source and info[\"id\"] == class_id:\n",
        "                # source.class_id combination already available, skip\n",
        "                return\n",
        "        # Add the class\n",
        "        self.class_info.append({\n",
        "            \"source\": source,\n",
        "            \"id\": class_id,\n",
        "            \"name\": class_name,\n",
        "        })\n",
        "\n",
        "    def add_image(self, source, image_id, path, **kwargs):\n",
        "        image_info = {\n",
        "            \"id\": image_id,\n",
        "            \"source\": source,\n",
        "            \"path\": path,\n",
        "        }\n",
        "        image_info.update(kwargs)\n",
        "        self.image_info.append(image_info)\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return a link to the image in its source Website or details about\n",
        "        the image that help looking it up or debugging it.\n",
        "\n",
        "        Override for your dataset, but pass to this function\n",
        "        if you encounter images not in your dataset.\n",
        "        \"\"\"\n",
        "        return \"\"\n",
        "\n",
        "    def prepare(self, class_map=None):\n",
        "        \"\"\"Prepares the Dataset class for use.\n",
        "\n",
        "        TODO: class map is not supported yet. When done, it should handle mapping\n",
        "              classes from different datasets to the same class ID.\n",
        "        \"\"\"\n",
        "\n",
        "        def clean_name(name):\n",
        "            \"\"\"Returns a shorter version of object names for cleaner display.\"\"\"\n",
        "            return \",\".join(name.split(\",\")[:1])\n",
        "\n",
        "        # Build (or rebuild) everything else from the info dicts.\n",
        "        self.num_classes = len(self.class_info)\n",
        "        self.class_ids = np.arange(self.num_classes)\n",
        "        self.class_names = [clean_name(c[\"name\"]) for c in self.class_info]\n",
        "        self.num_images = len(self.image_info)\n",
        "        self._image_ids = np.arange(self.num_images)\n",
        "\n",
        "        # Mapping from source class and image IDs to internal IDs\n",
        "        self.class_from_source_map = {\"{}.{}\".format(info['source'], info['id']): id\n",
        "                                      for info, id in zip(self.class_info, self.class_ids)}\n",
        "        self.image_from_source_map = {\"{}.{}\".format(info['source'], info['id']): id\n",
        "                                      for info, id in zip(self.image_info, self.image_ids)}\n",
        "\n",
        "        # Map sources to class_ids they support\n",
        "        self.sources = list(set([i['source'] for i in self.class_info]))\n",
        "        self.source_class_ids = {}\n",
        "        # Loop over datasets\n",
        "        for source in self.sources:\n",
        "            self.source_class_ids[source] = []\n",
        "            # Find classes that belong to this dataset\n",
        "            for i, info in enumerate(self.class_info):\n",
        "                # Include BG class in all datasets\n",
        "                if i == 0 or source == info['source']:\n",
        "                    self.source_class_ids[source].append(i)\n",
        "\n",
        "    def map_source_class_id(self, source_class_id):\n",
        "        \"\"\"Takes a source class ID and returns the int class ID assigned to it.\n",
        "\n",
        "        For example:\n",
        "        dataset.map_source_class_id(\"coco.12\") -> 23\n",
        "        \"\"\"\n",
        "        return self.class_from_source_map[source_class_id]\n",
        "\n",
        "    def get_source_class_id(self, class_id, source):\n",
        "        \"\"\"Map an internal class ID to the corresponding class ID in the source dataset.\"\"\"\n",
        "        info = self.class_info[class_id]\n",
        "        assert info['source'] == source\n",
        "        return info['id']\n",
        "\n",
        "    @property\n",
        "    def image_ids(self):\n",
        "        return self._image_ids\n",
        "\n",
        "    def source_image_link(self, image_id):\n",
        "        \"\"\"Returns the path or URL to the image.\n",
        "        Override this to return a URL to the image if it's available online for easy\n",
        "        debugging.\n",
        "        \"\"\"\n",
        "        return self.image_info[image_id][\"path\"]\n",
        "\n",
        "    def load_image(self, image_id):\n",
        "        \"\"\"Load the specified image and return a [H,W,3] Numpy array.\n",
        "        \"\"\"\n",
        "        # Load image\n",
        "        image = skimage.io.imread(self.image_info[image_id]['path'])\n",
        "        # If grayscale. Convert to RGB for consistency.\n",
        "        if image.ndim != 3:\n",
        "            image = skimage.color.gray2rgb(image)\n",
        "        # If has an alpha channel, remove it for consistency\n",
        "        if image.shape[-1] == 4:\n",
        "            image = image[..., :3]\n",
        "        return image\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Load instance masks for the given image.\n",
        "\n",
        "        Different datasets use different ways to store masks. Override this\n",
        "        method to load instance masks and return them in the form of am\n",
        "        array of binary masks of shape [height, width, instances].\n",
        "\n",
        "        Returns:\n",
        "            masks: A bool array of shape [height, width, instance count] with\n",
        "                a binary mask per instance.\n",
        "            class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # Override this function to load a mask from your dataset.\n",
        "        # Otherwise, it returns an empty mask.\n",
        "        logging.warning(\"You are using the default load_mask(), maybe you need to define your own one.\")\n",
        "        mask = np.empty([0, 0, 0])\n",
        "        class_ids = np.empty([0], np.int32)\n",
        "        return mask, class_ids\n",
        "\n",
        "class CocoDataset(Dataset):\n",
        "    def load_coco(self, dataset_dir, subset, class_ids=None, return_coco=False, auto_download=False):\n",
        "        \"\"\"Load a subset of the COCO dataset.\n",
        "        dataset_dir: The root directory of the COCO dataset.\n",
        "        subset: What to load (train, val, minival, valminusminival)\n",
        "        year: What dataset year to load (2014, 2017) as a string, not an integer\n",
        "        class_ids: If provided, only loads images that have the given classes.\n",
        "        class_map: TODO: Not implemented yet. Supports maping classes from\n",
        "            different datasets to the same class ID.\n",
        "        return_coco: If True, returns the COCO object.\n",
        "        auto_download: Automatically download and unzip MS-COCO images and annotations\n",
        "        \"\"\"\n",
        "        print(\"{}/annotations/instancesonly_filtered_gtFine_{}.json\".format(dataset_dir, subset))\n",
        "        coco = COCO(\"{}/annotations/instancesonly_filtered_gtFine_{}.json\".format(dataset_dir, subset))\n",
        "        if subset == \"minival\" or subset == \"valminusminival\":\n",
        "            subset = \"val\"\n",
        "        image_dir = dataset_dir #\"{}/{}\".format(dataset_dir, subset)\n",
        "\n",
        "\n",
        "        # Load all classes or a subset?\n",
        "        if not class_ids:\n",
        "            # All classes\n",
        "            class_ids = sorted(coco.getCatIds())\n",
        "\n",
        "        # All images or a subset?\n",
        "        if class_ids:\n",
        "            image_ids = []\n",
        "            for id in class_ids:\n",
        "                image_ids.extend(list(coco.getImgIds(catIds=[id])))\n",
        "            # Remove duplicates\n",
        "            image_ids = list(set(image_ids))\n",
        "        else:\n",
        "            # All images\n",
        "            image_ids = list(coco.imgs.keys())\n",
        "\n",
        "        # Add classes\n",
        "        for i in class_ids:\n",
        "            self.add_class(\"coco\", i, coco.loadCats(i)[0][\"name\"])\n",
        "\n",
        "        # Add images\n",
        "        for i in image_ids:\n",
        "            self.add_image(\n",
        "                \"coco\", image_id=i,\n",
        "                path=os.path.join(image_dir, coco.imgs[i]['file_name']),\n",
        "                width=coco.imgs[i][\"width\"],\n",
        "                height=coco.imgs[i][\"height\"],\n",
        "                annotations=coco.loadAnns(coco.getAnnIds(\n",
        "                    imgIds=[i], catIds=class_ids, iscrowd=None)))\n",
        "        if return_coco:\n",
        "            return coco\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Load instance masks for the given image.\n",
        "\n",
        "        Different datasets use different ways to store masks. This\n",
        "        function converts the different mask format to one format\n",
        "        in the form of a bitmap [height, width, instances].\n",
        "\n",
        "        Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # If not a COCO image, delegate to parent class.\n",
        "        image_info = self.image_info[image_id]\n",
        "        if image_info[\"source\"] != \"coco\":\n",
        "            return super(CocoDataset, self).load_mask(image_id)\n",
        "\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        annotations = self.image_info[image_id][\"annotations\"]\n",
        "        # Build mask of shape [height, width, instance_count] and list\n",
        "        # of class IDs that correspond to each channel of the mask.\n",
        "        for annotation in annotations:\n",
        "            class_id = self.map_source_class_id(\n",
        "                \"coco.{}\".format(annotation['category_id']))\n",
        "            if class_id:\n",
        "                m = self.annToMask(annotation, image_info[\"height\"],\n",
        "                                   image_info[\"width\"])\n",
        "                # Some objects are so small that they're less than 1 pixel area\n",
        "                # and end up rounded out. Skip those objects.\n",
        "                if m.max() < 1:\n",
        "                    continue\n",
        "                # Is it a crowd? If so, use a negative class ID.\n",
        "                if annotation['iscrowd']:\n",
        "                    # Use negative class ID for crowds\n",
        "                    class_id *= -1\n",
        "                    # For crowd masks, annToMask() sometimes returns a mask\n",
        "                    # smaller than the given dimensions. If so, resize it.\n",
        "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
        "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
        "                instance_masks.append(m)\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        # Pack instance masks into an array\n",
        "        if class_ids:\n",
        "            mask = np.stack(instance_masks, axis=2).astype(bool)\n",
        "            class_ids = np.array(class_ids, dtype=np.int32)\n",
        "            return mask, class_ids\n",
        "        else:\n",
        "            # Call super class to return an empty mask\n",
        "            return super(CocoDataset, self).load_mask(image_id)\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return a link to the image in the COCO Website.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"coco\":\n",
        "            return \"http://cocodataset.org/#explore?id={}\".format(info[\"id\"])\n",
        "        else:\n",
        "            super(CocoDataset, self).image_reference(image_id)\n",
        "\n",
        "    # The following two functions are from pycocotools with a few changes.\n",
        "\n",
        "    def annToRLE(self, ann, height, width):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        segm = ann['segmentation']\n",
        "        if isinstance(segm, list):\n",
        "            # polygon -- a single object might consist of multiple parts\n",
        "            # we merge all parts into one mask rle code\n",
        "            rles = maskUtils.frPyObjects(segm, height, width)\n",
        "            rle = maskUtils.merge(rles)\n",
        "        elif isinstance(segm['counts'], list):\n",
        "            # uncompressed RLE\n",
        "            rle = maskUtils.frPyObjects(segm, height, width)\n",
        "        else:\n",
        "            # rle\n",
        "            rle = ann['segmentation']\n",
        "        return rle\n",
        "\n",
        "    def annToMask(self, ann, height, width):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        rle = self.annToRLE(ann, height, width)\n",
        "        m = maskUtils.decode(rle)\n",
        "        return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roLplgV1FZCT"
      },
      "outputs": [],
      "source": [
        "# Visualization Utils (adopted from https://github.com/matterport/Mask_RCNN/tree/master)\n",
        "def extract_bboxes(mask):\n",
        "    \"\"\"Compute bounding boxes from masks.\n",
        "    mask: [height, width, num_instances]. Mask pixels are either 1 or 0.\n",
        "\n",
        "    Returns: bbox array [num_instances, (y1, x1, y2, x2)].\n",
        "    \"\"\"\n",
        "    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n",
        "    for i in range(mask.shape[-1]):\n",
        "        m = mask[:, :, i]\n",
        "        # Bounding box.\n",
        "        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n",
        "        vertical_indicies = np.where(np.any(m, axis=1))[0]\n",
        "        if horizontal_indicies.shape[0]:\n",
        "            x1, x2 = horizontal_indicies[[0, -1]]\n",
        "            y1, y2 = vertical_indicies[[0, -1]]\n",
        "            # x2 and y2 should not be part of the box. Increment by 1.\n",
        "            x2 += 1\n",
        "            y2 += 1\n",
        "        else:\n",
        "            # No mask for this instance. Might happen due to\n",
        "            # resizing or cropping. Set bbox to zeros\n",
        "            x1, x2, y1, y2 = 0, 0, 0, 0\n",
        "        boxes[i] = np.array([y1, x1, y2, x2])\n",
        "    return boxes.astype(np.int32)\n",
        "\n",
        "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n",
        "                   interpolation=None):\n",
        "    \"\"\"Display the given set of images, optionally with titles.\n",
        "    images: list or array of image tensors in HWC format.\n",
        "    titles: optional. A list of titles to display with each image.\n",
        "    cols: number of images per row\n",
        "    cmap: Optional. Color map to use. For example, \"Blues\".\n",
        "    norm: Optional. A Normalize instance to map values to colors.\n",
        "    interpolation: Optional. Image interpolation to use for display.\n",
        "    \"\"\"\n",
        "    titles = titles if titles is not None else [\"\"] * len(images)\n",
        "    rows = len(images) // cols + 1\n",
        "    plt.figure(figsize=(14, 14 * rows // cols))\n",
        "    i = 1\n",
        "    for image, title in zip(images, titles):\n",
        "        plt.subplot(rows, cols, i)\n",
        "        plt.title(title, fontsize=9)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(image.astype(np.uint8), cmap=cmap,\n",
        "                   norm=norm, interpolation=interpolation)\n",
        "        i += 1\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def display_top_masks(image, mask, class_ids, class_names, limit=4):\n",
        "    \"\"\"Display the given image and the top few class masks.\"\"\"\n",
        "    to_display = []\n",
        "    titles = []\n",
        "    to_display.append(image)\n",
        "    titles.append(\"H x W={}x{}\".format(image.shape[0], image.shape[1]))\n",
        "    # Pick top prominent classes in this image\n",
        "    unique_class_ids = np.unique(class_ids)\n",
        "    mask_area = [np.sum(mask[:, :, np.where(class_ids == i)[0]])\n",
        "                 for i in unique_class_ids]\n",
        "    top_ids = [v[0] for v in sorted(zip(unique_class_ids, mask_area),\n",
        "                                    key=lambda r: r[1], reverse=True) if v[1] > 0]\n",
        "    # Generate images and titles\n",
        "    for i in range(limit):\n",
        "        class_id = top_ids[i] if i < len(top_ids) else -1\n",
        "        # Pull masks of instances belonging to the same class.\n",
        "        m = mask[:, :, np.where(class_ids == class_id)[0]]\n",
        "        m = np.sum(m * np.arange(1, m.shape[-1] + 1), -1)\n",
        "        to_display.append(m)\n",
        "        titles.append(class_names[class_id] if class_id != -1 else \"-\")\n",
        "    display_images(to_display, titles=titles, cols=limit + 1, cmap=\"Blues_r\")\n",
        "\n",
        "def random_colors(N, bright=True):\n",
        "    \"\"\"\n",
        "    Generate random colors.\n",
        "    To get visually distinct colors, generate them in HSV space then\n",
        "    convert to RGB.\n",
        "    \"\"\"\n",
        "    brightness = 1.0 if bright else 0.7\n",
        "    hsv = [(i / N, 1, brightness) for i in range(N)]\n",
        "    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
        "    random.shuffle(colors)\n",
        "    return colors\n",
        "\n",
        "\n",
        "def apply_mask(image, mask, color, alpha=0.5):\n",
        "    \"\"\"Apply the given mask to the image.\n",
        "    \"\"\"\n",
        "    for c in range(3):\n",
        "        image[:, :, c] = np.where(mask == 1,\n",
        "                                  image[:, :, c] *\n",
        "                                  (1 - alpha) + alpha * color[c] * 255,\n",
        "                                  image[:, :, c])\n",
        "    return image\n",
        "\n",
        "\n",
        "def display_instances(image, boxes, masks, class_ids, class_names,\n",
        "                      scores=None, title=\"\",\n",
        "                      figsize=(16, 8), ax=None,\n",
        "                      show_mask=True, show_bbox=True,\n",
        "                      colors=None, captions=None, save_path=None):\n",
        "    \"\"\"\n",
        "    boxes: [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.\n",
        "    masks: [height, width, num_instances]\n",
        "    class_ids: [num_instances]\n",
        "    class_names: list of class names of the dataset\n",
        "    scores: (optional) confidence scores for each box\n",
        "    title: (optional) Figure title\n",
        "    show_mask, show_bbox: To show masks and bounding boxes or not\n",
        "    figsize: (optional) the size of the image\n",
        "    colors: (optional) An array or colors to use with each object\n",
        "    captions: (optional) A list of strings to use as captions for each object\n",
        "    \"\"\"\n",
        "    # Number of instances\n",
        "    N = boxes.shape[0]\n",
        "    if not N:\n",
        "        print(\"\\n*** No instances to display *** \\n\")\n",
        "    else:\n",
        "        assert boxes.shape[0] == masks.shape[-1] == class_ids.shape[0]\n",
        "\n",
        "    # If no axis is passed, create one and automatically call show()\n",
        "    auto_show = False\n",
        "    if not ax:\n",
        "        _, ax = plt.subplots(1, figsize=figsize)\n",
        "        auto_show = True\n",
        "\n",
        "    # Generate random colors\n",
        "    colors = colors or random_colors(N)\n",
        "\n",
        "    # Show area outside image boundaries.\n",
        "    height, width = image.shape[:2]\n",
        "    ax.set_ylim(height + 10, -10)\n",
        "    ax.set_xlim(-10, width + 10)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(title)\n",
        "\n",
        "    masked_image = image.astype(np.uint32).copy()\n",
        "    for i in range(N):\n",
        "        color = colors[i]\n",
        "\n",
        "        # Bounding box\n",
        "        if not np.any(boxes[i]):\n",
        "            # Skip this instance. Has no bbox. Likely lost in image cropping.\n",
        "            continue\n",
        "        y1, x1, y2, x2 = boxes[i]\n",
        "        if show_bbox:\n",
        "            p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
        "                                alpha=0.7, linestyle=\"dashed\",\n",
        "                                edgecolor=color, facecolor='none')\n",
        "            ax.add_patch(p)\n",
        "\n",
        "        # Label\n",
        "        if not captions:\n",
        "            class_id = class_ids[i]\n",
        "            score = scores[i] if scores is not None else None\n",
        "            label = class_names[class_id]\n",
        "            caption = \"{} {:.3f}\".format(label, score) if score else label\n",
        "        else:\n",
        "            caption = captions[i]\n",
        "        ax.text(x1, y1 + 8, caption,\n",
        "                color='w', size=11, backgroundcolor=\"none\")\n",
        "\n",
        "        # Mask\n",
        "        mask = masks[:, :, i]\n",
        "        if show_mask:\n",
        "            masked_image = apply_mask(masked_image, mask, color)\n",
        "\n",
        "        # Mask Polygon\n",
        "        # Pad to ensure proper polygons for masks that touch image edges.\n",
        "        padded_mask = np.zeros(\n",
        "            (mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
        "        padded_mask[1:-1, 1:-1] = mask\n",
        "        contours = find_contours(padded_mask, 0.5)\n",
        "        for verts in contours:\n",
        "            # Subtract the padding and flip (y, x) to (x, y)\n",
        "            verts = np.fliplr(verts) - 1\n",
        "            p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n",
        "            ax.add_patch(p)\n",
        "    ax.imshow(masked_image.astype(np.uint8))\n",
        "\n",
        "    if save_path: plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    if auto_show:\n",
        "        pass\n",
        "        #plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZinNRxt1jlSA"
      },
      "outputs": [],
      "source": [
        "def visualize_instances():\n",
        "    # load cityscapes as COCO\n",
        "    dataset = CocoDataset()\n",
        "    dataset.load_coco(\"data/cityscapes\", \"train\")\n",
        "    dataset.prepare()\n",
        "\n",
        "    print(\"Image Count: {}\".format(len(dataset.image_ids)))\n",
        "    print(\"Class Count: {}\".format(dataset.num_classes))\n",
        "    for i, info in enumerate(dataset.class_info):\n",
        "        print(\"{:3}. {:50}\".format(i, info['name']))\n",
        "    # plot masks for each class\n",
        "    for _ in range(3):\n",
        "        random_image_id = random.choice(dataset.image_ids)\n",
        "        image = dataset.load_image(random_image_id)\n",
        "        mask, class_ids = dataset.load_mask(random_image_id)\n",
        "        display_top_masks(image, mask, class_ids, dataset.class_names)\n",
        "    # Plot display instances\n",
        "    for _ in range(3):\n",
        "        random_image_id = random.choice(dataset.image_ids)\n",
        "        image = dataset.load_image(random_image_id)\n",
        "        mask, class_ids = dataset.load_mask(random_image_id)\n",
        "        bbox = extract_bboxes(mask)\n",
        "        display_instances(image, bbox, mask, class_ids, dataset.class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vYAEHZVcjABJ",
        "outputId": "2c7d5954-1aaf-48ad-a90c-fb729fdf0c60"
      },
      "outputs": [],
      "source": [
        "visualize_instances()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAytLXgVwbtv",
        "outputId": "9f5a9017-02f6-4e34-cf0c-135c3ecce18e"
      },
      "outputs": [],
      "source": [
        "%cd /content/CV-final-project/conversion/data/cityscapes\n",
        "!mv gtFine filtered_gtFine\n",
        "%cd /content/CV-final-project/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhfY_2sij6Ee"
      },
      "source": [
        "# Modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tppkYeYYWWtq"
      },
      "outputs": [],
      "source": [
        "# Checkpoint with pretrained weights\n",
        "checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n",
        "            map_location='cpu',\n",
        "            check_hash=True)\n",
        "\n",
        "# Remove class weights\n",
        "del checkpoint[\"model\"][\"class_embed.weight\"]\n",
        "del checkpoint[\"model\"][\"class_embed.bias\"]\n",
        "\n",
        "# Save\n",
        "torch.save(checkpoint,\n",
        "           'detr-r50_no-class-head.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiWFFjSuyNEL"
      },
      "outputs": [],
      "source": [
        "# example training instace\n",
        "!python main.py \\\n",
        "  --dataset_file \"cityscapes\" \\\n",
        "  --coco_path \"/content/CV-final-project/conversion/data/cityscapes\" \\\n",
        "  --output_dir \"outputs\" \\\n",
        "  --resume \"detr-r50_no-class-head.pth\" \\\n",
        "  --num_classes 9 \\\n",
        "  --epochs 10 \\\n",
        "  --lr  1e-4 \\\n",
        "  --batch_size 2 \\\n",
        "  --weight_decay 1e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIDLyAASfZHT"
      },
      "source": [
        "# Graph / Visualize Performance + Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNttPTGveNn8",
        "outputId": "2bdf41e8-9cfd-45ff-ceef-b6ed990977cc"
      },
      "outputs": [],
      "source": [
        "from util.plot_utils import plot_logs\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "log_directory = [Path('outputs/')]\n",
        "fields_of_interest = (\n",
        "    'loss',\n",
        "    'mAP',\n",
        "    )\n",
        "\n",
        "plot_logs(log_directory,\n",
        "          fields_of_interest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6FQz0NyeLtE"
      },
      "outputs": [],
      "source": [
        "#visualize an example using the finetuned model\n",
        "model = torch.hub.load('facebookresearch/detr',\n",
        "                       'detr_resnet50',\n",
        "                       pretrained=False,\n",
        "                       num_classes=9)\n",
        "\n",
        "checkpoint = torch.load('outputs/checkpoint.pth',\n",
        "                        map_location='cpu')\n",
        "\n",
        "model.load_state_dict(checkpoint['model'],\n",
        "                      strict=False)\n",
        "\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynreNa00dSLA"
      },
      "outputs": [],
      "source": [
        "categories = [\n",
        "'rider','person','truck','car','bus','train','motorcycle','bicycle'\n",
        "]\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# standard PyTorch mean-std input image normalization\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "\n",
        "def filter_bboxes_from_outputs(outputs,\n",
        "                               threshold=0.7):\n",
        "\n",
        "  # keep only predictions with confidence above threshold\n",
        "  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "  keep = probas.max(-1).values > threshold\n",
        "\n",
        "  probas_to_keep = probas[keep]\n",
        "\n",
        "  # convert boxes from [0; 1] to image scales\n",
        "  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
        "\n",
        "  return probas_to_keep, bboxes_scaled, probas, keep\n",
        "\n",
        "def plot_finetuned_results(pil_img, prob=None, boxes=None):\n",
        "    plt.figure(figsize=(16,10))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    colors = COLORS * 100\n",
        "    if prob is not None and boxes is not None:\n",
        "      for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
        "          ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                    fill=False, color=c, linewidth=3))\n",
        "          cl = p.argmax()\n",
        "          text = f'{categories[cl]}: {p[cl]:0.2f}'\n",
        "          ax.text(xmin, ymin, text, fontsize=15,\n",
        "                  bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def run_worflow(my_image, my_model):\n",
        "  # mean-std normalize the input image (batch-size: 1)\n",
        "  img = transform(my_image).unsqueeze(0)\n",
        "\n",
        "  # propagate through the model\n",
        "  outputs = my_model(img)\n",
        "\n",
        "  for threshold in [0.9, 0.7]:\n",
        "\n",
        "    probas_to_keep, bboxes_scaled, probas, keep = filter_bboxes_from_outputs(outputs,\n",
        "                                                              threshold=threshold)\n",
        "\n",
        "    plot_finetuned_results(my_image,\n",
        "                           probas_to_keep,\n",
        "                           bboxes_scaled)\n",
        "\n",
        "\n",
        "  return probas_to_keep, bboxes_scaled, probas, keep\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "img_name = '/content/people.jpeg' ### directly upload the file to colab folder\n",
        "im = Image.open(img_name)\n",
        "\n",
        "probas_to_keep, bboxes_scaled, probas, keep = run_worflow(im, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3MKioDQuCKt"
      },
      "outputs": [],
      "source": [
        "conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n",
        "\n",
        "hooks = [\n",
        "    model.backbone[-2].register_forward_hook(\n",
        "        lambda self, input, output: conv_features.append(output)\n",
        "    ),\n",
        "    model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n",
        "        lambda self, input, output: enc_attn_weights.append(output[1])\n",
        "    ),\n",
        "    model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
        "        lambda self, input, output: dec_attn_weights.append(output[1])\n",
        "    ),\n",
        "]\n",
        "\n",
        "img = transform(im).unsqueeze(0)\n",
        "outputs = model(img)\n",
        "\n",
        "for hook in hooks:\n",
        "    hook.remove()\n",
        "\n",
        "conv_features = conv_features[0]\n",
        "enc_attn_weights = enc_attn_weights[0]\n",
        "dec_attn_weights = dec_attn_weights[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr4DLp03uKbj"
      },
      "outputs": [],
      "source": [
        "# get the feature map shape\n",
        "h, w = conv_features['0'].tensors.shape[-2:]\n",
        "\n",
        "fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(10, 7))\n",
        "colors = COLORS * 100\n",
        "for idx, ax_i, (xmin, ymin, xmax, ymax) in zip(keep.nonzero(), axs.T, bboxes_scaled):\n",
        "    ax = ax_i[0]\n",
        "    #ax.imshow(dec_attn_weights[0, idx].view(h, w))\n",
        "    ax.imshow(dec_attn_weights[0, idx].detach().cpu().view(h, w).numpy())\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'query id: {idx.item()}')\n",
        "    ax = ax_i[1]\n",
        "    ax.imshow(im)\n",
        "    idx_value = idx.item()\n",
        "    print(idx_value)\n",
        "    xmin, ymin, xmax, ymax = bboxes_scaled[idx_value].detach().cpu().numpy()\n",
        "    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                               fill=False, color='blue', linewidth=3))\n",
        "    ax.axis('off')\n",
        "    ax.set_title(categories[probas[idx].argmax()])\n",
        "fig.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
