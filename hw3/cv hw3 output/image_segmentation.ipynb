{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "In this assignment, you will practice building and training Convolutional Neural Networks with Pytorch to solve computer vision tasks.  This assignment includes two sections, each involving different tasks:\n",
        "\n",
        "(1) Image Classification. Predict image-level category labels on two historically notable image datasets: **CIFAR-10** and **MNIST**.\n",
        "\n",
        "(2) Image Segmentation. Predict pixel-wise classification (semantic segmentation) on synthetic input images formed by superimposing MNIST images on top of CIFAR images.\n",
        "\n",
        "You will design your own models in each section and build the entire training/testing pipeline with PyTorch.\n",
        "PyTorch provides optimized implementations of the building blocks and additional utilities, both of which will be necessary for experiments on real datasets. It is highly recommended to read the official [documentation](https://pytorch.org/docs/stable/index.html) and [examples](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) before starting your implementation. There are some APIs that you'll find useful:\n",
        "[Layers](http://pytorch.org/docs/stable/nn.html),\n",
        "[Activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity),\n",
        "[Loss functions](http://pytorch.org/docs/stable/nn.html#loss-functions),\n",
        "[Optimizers](http://pytorch.org/docs/stable/optim.html)\n",
        "\n",
        "It is highly recommended to use Google Colab and run the notebook on a GPU node. Check https://colab.research.google.com/ and look for tutorials online. To use a GPU go to Runtime -> Change runtime type and select GPU.\n"
      ],
      "metadata": {
        "id": "PgO0k03dlZ-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (2) Image Segmentation\n",
        "The task consists of performing pixel-wise classification on a synthetic dataset\n",
        "of 32x32 RGB images. Each image was generated by placing a MNIST sample (a grayscale\n",
        "image of a digit between 0 and 9) on top of a CIFAR-10 sample (a RGB image drawn from\n",
        "one of 10 possible classes). Each image has an accompanying target tensor of size 32x32,\n",
        "where in each pixel location (i,j) it contains the ground-truth label of the MNIST digit\n",
        "(ranging from 0 to 9) or of the CIFAR-10 image (ranging from 10 to 19), depending on whether\n",
        "the (i,j) pixel in the original image belongs to the superposed MNIST image or not. The\n",
        "metric of interest here is pixel-wise accuracy, which is the fraction of pixels in each image\n",
        "for which your model predicted the correct class (out of a total of 20 classes, as described\n",
        "above).\n",
        "\n",
        "Note that there are many ways to frame the above task. For example, your CNN can directly\n",
        "output a 20x32x32 tensor for each input image, representing a distribution over the possible\n",
        "20 classes for each of the 32x32 pixels. However, note that the problem has a lot of additional\n",
        "structure: for example, each 32x32 target tensor only has two distinct numbers in it, the label\n",
        "of the MNIST digit and the label of the CIFAR-10 background image -- accounting for such\n",
        "structure will make training faster and likely improve your model's final performance. Your\n",
        "model should be able to achieve around 70% accuracy on the test set when trained for 100 epochs.\n",
        "\n",
        "To finish this section step by step, you need to:\n",
        "\n",
        "* Prepare data by building a dataset and data loader. (already provided below)\n",
        "\n",
        "* Implement training code (6 points) & testing code (6 points), including saving and loading of models.\n",
        "\n",
        "* Construct a model (12 points) and choose an optimizer (3 points).\n",
        "\n",
        "* Describe what you did, any additional features you implemented, and/or any graphs you made in training and evaluating your network. Report final test accuracy @100 epochs in a writeup: hw3.pdf (3 points)"
      ],
      "metadata": {
        "id": "b5lueGqqw6sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import sampler\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "#NOTE: manually added data loading utils for collab environment fix\n",
        "#from utils import SegDataset $\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "class SegDataset(Dataset):\n",
        "    def __init__(self, root_path='./data', train=True, transform=None):\n",
        "        super(SegDataset, self).__init__()\n",
        "        self.transform = transform\n",
        "\n",
        "        try:\n",
        "            if train:\n",
        "                self.data = torch.load(os.path.join(root_path, \"train_data.pth\")).float() / 255.\n",
        "                self.target = torch.load(os.path.join(root_path, \"train_target.pth\")).long()\n",
        "            else:\n",
        "                self.data = torch.load(os.path.join(root_path, \"test_data.pth\")).float() / 255.\n",
        "                self.target = torch.load(os.path.join(root_path, \"test_target.pth\")).long()\n",
        "        except:\n",
        "            print(\"Error when loading data: {train, test}_{data, target}.pth should be in data folder.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data[idx]\n",
        "        target = self.target[idx]\n",
        "        if self.transform is not None:\n",
        "            data = self.transform(data)\n",
        "        return data, target\n"
      ],
      "metadata": {
        "id": "w2DHjbo3SD-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation:\n",
        "\n",
        "Setup a Dataset for training and testing.\n",
        "\n",
        "Datasets load single training examples one a time, so we practically wrap each Dataset in a DataLoader, which loads a data batch in parallel."
      ],
      "metadata": {
        "id": "QpFf3VOO1bT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seg_train = SegDataset(root_path='./data', train=True, transform=None) # in my testing in collab I modified ./data to ., as I was loading .pth data files manually into collab\n",
        "loader_train = DataLoader(seg_train, batch_size=64, shuffle=True)\n",
        "seg_test = SegDataset(root_path='./data', train=False, transform=None)\n",
        "loader_test = DataLoader(seg_test, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "R_6rGLHwzMxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Design/choose your own model structure (12 points) and optimizer (3 points).\n",
        "You might want to adjust following configurations for better performance:\n",
        "\n",
        "(1) Network architecture:\n",
        "- You can borrow some ideas from existing convnets design, e.g., [ResNet](https://arxiv.org/abs/1512.03385) where\n",
        "the input from the previous layer is added to the output, or [UNet](https://arxiv.org/pdf/1505.04597.pdf) where you can stack intermediate features from previous layers.\n",
        "- Note: Do not **directly copy** an existing network design.\n",
        "\n",
        "(2) Architecture hyperparameters:\n",
        "- Filter size, number of filters, and number of layers (depth). Make careful choices to tradeoff computational efficiency and accuracy.\n",
        "- Pooling vs. Strided Convolution\n",
        "- Batch normalization\n",
        "- Choice of non-linear activation\n",
        "\n",
        "(3) Choice of optimizer (e.g., SGD, Adam, Adagrad, RMSprop) and associated hyperparameters (e.g., learning rate, momentum).\n"
      ],
      "metadata": {
        "id": "58GUecr83BmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resblock architecture with striding\n",
        "# same ResBlock architecture from CIFAR10 implimentation\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResBlock, self).__init__()\n",
        "        # convolution / batch norm blocks\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = nn.Sequential()\n",
        "        # strided downsampling\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.downsample(identity)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class PixelWiseClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PixelWiseClassifier, self).__init__()\n",
        "        # convolution, batchnorm, resblock layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.resblock1 = ResBlock(64, 64)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.resblock2 = ResBlock(128, 128)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.resblock3 = ResBlock(256, 256)\n",
        "\n",
        "        # simple convolution, batch norm layers to add model strength\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv6 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # another convolution, batchnorm, resblock layers\n",
        "        self.resblock4 = ResBlock(512, 512)\n",
        "        self.conv7 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # another convolution, batch norm layers\n",
        "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.resblock5 = ResBlock(512, 512)\n",
        "        self.conv9 = nn.Conv2d(512, 20, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.resblock1(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.resblock2(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.resblock3(x)\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = self.resblock4(x)\n",
        "        x = F.relu(self.bn7(self.conv7(x)))\n",
        "        x = F.relu(self.bn8(self.conv8(x)))\n",
        "        x = self.resblock5(x)\n",
        "        x = self.conv9(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "4ZA5kw3X3qLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training (6 points)\n",
        "\n",
        "Train a model on the given dataset using the PyTorch Module API.\n",
        "\n",
        "Inputs:\n",
        "- loader_train: The loader from which train samples will be drawn from.\n",
        "- loader_test: The loader from which test samples will be drawn from\n",
        "- model: A PyTorch Module giving the model to train.\n",
        "- optimizer: An Optimizer object we will use to train the model\n",
        "- epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "\n",
        "Returns: Nothing, but prints model accuracies during training."
      ],
      "metadata": {
        "id": "MOuB58GA5W_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(loader_train, loader_test, model, optimizer, epochs=100):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    # standard cross entropy loss function\n",
        "    criterion = F.cross_entropy\n",
        "\n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        for t, (x, y) in enumerate(loader_train):\n",
        "            # (1) move data to GPU\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # (2) forward and get loss\n",
        "            output = model(x)\n",
        "            loss = criterion(output, y)\n",
        "\n",
        "            # (3) zero out all of the gradients for the variables which the optimizer\n",
        "            # will update.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # (4) the backwards pass: compute the gradient of the loss with\n",
        "            # respect to each  parameter of the model.\n",
        "            loss.backward()\n",
        "\n",
        "            # (5) update the parameters of the model using the gradients\n",
        "            # computed by the backwards pass.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Only print out every 25 steps\n",
        "            if t % 25 == 0:\n",
        "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
        "\n",
        "        test(loader_test, model)\n",
        "\n",
        "        # save model for partial model training functionality\n",
        "        torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "0Kx_q5rv5ddg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing (6 points)\n",
        "Test a model using the PyTorch Module API.\n",
        "\n",
        "Inputs:\n",
        "- loader: The loader from which test samples will be drawn from.\n",
        "- model: A PyTorch Module giving the model to test.\n",
        "\n",
        "Returns: Nothing, but prints model accuracies during training."
      ],
      "metadata": {
        "id": "bdxEMZpC6E9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval() # set model to evaluation mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            # (1) move to GPU\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # (2) forward and calculate scores and predictions\n",
        "            output = model(x)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "            # (3) accumulate num_correct and num_samples\n",
        "            num_samples += y.size(0) * y.size(1) * y.size(2)\n",
        "            num_correct += (predicted == y).sum().item()\n",
        "\n",
        "\n",
        "    acc = num_correct / num_samples\n",
        "    print('Eval %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
      ],
      "metadata": {
        "id": "KA5YBjxx6IeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model util for partial model training when model is saved\n",
        "def load_model(model, load_path):\n",
        "    model.load_state_dict(torch.load(load_path))\n",
        "    print(f\"Model loaded from {load_path}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "1POgdNB3i5Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Describe your design details in the writeup hw3.pdf. (3 points)\n",
        "\n",
        "Finish your model and optimizer below."
      ],
      "metadata": {
        "id": "omwoZPA26V-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Segmentation Implementation"
      ],
      "metadata": {
        "id": "fBPbLdk1Ha1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01 # standard learning rate, momentum, decay rate\n",
        "momentum = 0.9\n",
        "weight_decay = 0.01\n",
        "model = PixelWiseClassifier()\n",
        "optimizer = optim.SGD(model.parameters(), lr, momentum, weight_decay)\n",
        "train(loader_train, loader_test, model, optimizer, epochs=100)\n",
        "\n",
        "# loaded_model = load_model(PixelWiseClassifier(), \"model.pth\") #example model loading for partial training"
      ],
      "metadata": {
        "id": "jM6GIIFP6XhG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}